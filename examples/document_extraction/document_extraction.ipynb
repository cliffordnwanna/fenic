{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Metadata Extraction with Fenic\n",
    "\n",
    "This notebook demonstrates how to extract structured metadata from unstructured document text using fenic's semantic operations. Fenic's structured extraction functionality leverages large language models to intelligently parse and extract structured information from diverse document types including research papers, product announcements, meeting notes, news articles, and technical documentation.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Setting up fenic sessions with semantic capabilities\n",
    "- Creating DataFrames from document data\n",
    "- Extracting structured metadata using AI-powered operations\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, we need to import the necessary libraries and configure our fenic session. We'll set up:\n",
    "\n",
    "- **Type hints** from `typing` for better code documentation\n",
    "- **Pydantic** for our second extraction approach \n",
    "- **Fenic** as our main DataFrame library\n",
    "\n",
    "For the session configuration, we're setting up semantic capabilities using OpenAI's GPT-4o-mini model with specific rate limits:\n",
    "- **RPM (Requests Per Minute)**: 500 requests\n",
    "- **TPM (Tokens Per Minute)**: 200,000 tokens\n",
    "\n",
    "This configuration ensures we can efficiently process our document extraction tasks while staying within API limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "from pydantic import BaseModel, Field\n",
    "import fenic as fc\n",
    "\n",
    "# Configure session with semantic capabilities\n",
    "config = fc.SessionConfig(\n",
    "        app_name=\"document_extraction\",\n",
    "        semantic=fc.SemanticConfig(\n",
    "            language_models={\n",
    "                \"mini\": fc.OpenAIModelConfig(\n",
    "                    model_name=\"gpt-4o-mini\",\n",
    "                    rpm=500,\n",
    "                    tpm=200_000,\n",
    "                )\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Create session\n",
    "session = fc.Session.get_or_create(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Document Data\n",
    "\n",
    "Now let's create our test dataset. We've carefully selected 5 diverse document types to showcase the versatility of metadata extraction:\n",
    "\n",
    "1. **Research Paper** (`doc_001`) - Academic study on neural networks and climate prediction\n",
    "2. **Product Announcement** (`doc_002`) - CloudSync Pro file synchronization software launch\n",
    "3. **Meeting Notes** (`doc_003`) - Engineering team standup with decisions and action items\n",
    "4. **News Article** (`doc_004`) - Breaking news about a data breach incident\n",
    "5. **Technical Documentation** (`doc_005`) - API reference for an authentication service\n",
    "\n",
    "Each document contains different types of metadata (titles, dates, keywords, etc.) that we'll extract automatically. After creating the DataFrame, we'll inspect the basic properties including document IDs and text lengths to understand our data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_data = [\n",
    "        {\n",
    "            \"id\": \"doc_001\",\n",
    "            \"text\": \"Neural Networks for Climate Prediction: A Comprehensive Study. Published March 15, 2024. This research presents a novel deep learning approach for predicting climate patterns using multi-layered neural networks. Our methodology combines satellite imagery data with ground-based sensor readings to achieve 94% accuracy in temperature forecasting. The study was conducted over 18 months across 12 research stations. Keywords: machine learning, climate modeling, neural networks, environmental science.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc_002\",\n",
    "            \"text\": \"Introducing CloudSync Pro - Next-Generation File Synchronization. Release Date: January 8, 2024. CloudSync Pro revolutionizes how teams collaborate with real-time file synchronization across unlimited devices. Features include end-to-end encryption, automatic conflict resolution, and integration with over 50 productivity tools. Pricing starts at $12/month per user with enterprise discounts available. Contact our sales team for a personalized demo.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc_003\",\n",
    "            \"text\": \"Weekly Engineering Standup - December 4, 2023. Attendees: Sarah Chen (Lead), Marcus Rodriguez (Backend), Lisa Park (Frontend), James Wilson (DevOps). Key decisions: Migration to Kubernetes approved for Q1 2024, new CI/CD pipeline reduces deployment time by 60%, API rate limiting implementation scheduled for next sprint. Action items: Sarah to finalize container specifications, Marcus to document database migration plan.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc_004\",\n",
    "            \"text\": \"Breaking: Major Data Breach Affects 2.3 Million Users. December 12, 2023 - TechCorp announced today that unauthorized access to customer databases occurred between November 28-30, 2023. Compromised data includes email addresses, encrypted passwords, and partial payment information. The company has implemented additional security measures and is offering free credit monitoring to affected users. Stock prices dropped 8% in after-hours trading.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc_005\",\n",
    "            \"text\": \"API Reference: Authentication Service v2.1. Last updated: February 20, 2024. The Authentication Service provides secure user login and session management for distributed applications. Supports OAuth 2.0, SAML, and multi-factor authentication. Rate limits: 1000 requests per hour for standard accounts, 10000 for premium. Available endpoints include /auth/login, /auth/refresh, /auth/logout. Response format: JSON with standardized error codes.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Create DataFrame\n",
    "docs_df = session.create_dataframe(documents_data)\n",
    "\n",
    "docs_df.select(\"id\", fc.text.length(\"text\").alias(\"text_length\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Extraction\n",
    "\n",
    "Fenic enables structured data extraction using LLMs by leveraging Pydantic models to define rich, schema-driven metadata extraction workflows. Here's how it works:\n",
    "\n",
    "1. **Schema Definition with Pydantic**\n",
    "\n",
    "   Define a Pydantic model to represent the structure of the data you want to extract. Each field must include a natural language description. This schema drives prompt generation and model output parsing.\n",
    "\n",
    "2. **LLM Orchestration**\n",
    "\n",
    "   Fenic uses the model provider of your choice to call the LLM with a structured output or tool-calling interface. The LLM returns data that conforms to the schema you defined.\n",
    "\n",
    "3. **Data Structuring**\n",
    "\n",
    "   The extracted data is represented as a struct column in a DataFrame with native Fenic struct fields. From there, it can be:\n",
    "\n",
    "   - Unnested into individual columns\n",
    "   - Exploded if it contains arrays\n",
    "   - Processed in place as nested data\n",
    "\n",
    "Because Fenic maps Pydantic models to a strongly typed, columnar data model, certain Python types are not currently supported:\n",
    "\n",
    "- **Non-Optional Union types**: Not expressible in Fenic's type system\n",
    "- **Dictionaries**: Fenic does not yet support map types (future support via a JsonType is planned)\n",
    "- **Custom classes / dataclasses**: These are stateful or logic-heavy constructs that don't fit the declarative nature of Fenic's data model\n",
    "\n",
    "Despite these constraints, you can define complex extraction schemas using nested Pydantic models, optional fields, and listsâ€”enabling robust and expressive structured extraction pipelines.\n",
    "\n",
    "Let's see it in action on the documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic model for document metadata\n",
    "class DocumentMetadata(BaseModel):\n",
    "    \"\"\"Pydantic model for document metadata extraction.\"\"\"\n",
    "    title: str = Field(description=\"The main title or subject of the document\")\n",
    "    document_type: Literal[\"research paper\", \"product announcement\", \"meeting notes\", \"news article\", \"technical documentation\", \"other\"] = Field(description=\"Type of document\")\n",
    "    date: str = Field(description=\"Any date mentioned in the document (publication date, meeting date, etc.)\")\n",
    "    keywords: List[str] = Field(description=\"List of key topics, technologies, or important terms mentioned in the document\")\n",
    "    summary: str = Field(description=\"Brief one-sentence summary of the document's main purpose or content\")\n",
    "\n",
    "# Apply extraction using Pydantic model\n",
    "pydantic_extracted_df = docs_df.select(\n",
    "    \"id\",\n",
    "    fc.semantic.extract(\"text\", DocumentMetadata).alias(\"metadata\")\n",
    ")\n",
    "\n",
    "# Flatten the extracted metadata into separate columns\n",
    "pydantic_results = pydantic_extracted_df.select(\n",
    "    \"id\",\n",
    "    pydantic_extracted_df.metadata.title.alias(\"title\"),\n",
    "    pydantic_extracted_df.metadata.document_type.alias(\"document_type\"),\n",
    "    pydantic_extracted_df.metadata.date.alias(\"date\"),\n",
    "    pydantic_extracted_df.metadata.keywords.alias(\"keywords\"),\n",
    "    pydantic_extracted_df.metadata.summary.alias(\"summary\")\n",
    ")\n",
    "\n",
    "pydantic_results.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Conclusion\n",
    "\n",
    "Finally, we properly close our fenic session to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
